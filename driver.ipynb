{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f93030d",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition with PyTorch\n",
    "\n",
    "This notebook demonstrates real-time facial emotion recognition using a PyTorch model. The application captures video from a webcam, detects faces, and predicts the emotion displayed by each face.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.6+\n",
    "- PyTorch\n",
    "- OpenCV (cv2)\n",
    "- NumPy\n",
    "- A webcam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f977420",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a93d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b5ecc",
   "metadata": {},
   "source": [
    "## Load the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc762d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Path to the model file\n",
    "model_path = 'models/model.pth'\n",
    "\n",
    "# Check if model file exists\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f'Model file not found at {model_path}')\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print('Model loaded successfully')\n",
    "except Exception as e:\n",
    "    print(f'Error loading model: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36e54e",
   "metadata": {},
   "source": [
    "## Define Emotion Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ff65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion labels (ensure they match the order used in training)\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "print(f'Emotion classes: {emotion_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bafa5",
   "metadata": {},
   "source": [
    "## Real-time Emotion Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedaa146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotions():\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Check if webcam opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print('Error: Could not open webcam')\n",
    "        return\n",
    "    \n",
    "    # Load face detection model\n",
    "    try:\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    except Exception as e:\n",
    "        print(f'Error loading face cascade classifier: {e}')\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    print('Starting emotion detection. Press q to exit.')\n",
    "    \n",
    "    # FPS calculation variables\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        # Read frame from webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print('Error: Failed to capture frame')\n",
    "            break\n",
    "        \n",
    "        # Mirror the frame for more intuitive display\n",
    "        mirrored_frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(mirrored_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "        \n",
    "        # Process each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract face region\n",
    "            face_roi = gray[y:y + h, x:x + w]\n",
    "            \n",
    "            # Preprocess for PyTorch model\n",
    "            face_resized = cv2.resize(face_roi, (48, 48))\n",
    "            \n",
    "            # Convert to PyTorch tensor and normalize\n",
    "            face_tensor = torch.from_numpy(face_resized).float() / 255.0\n",
    "            face_tensor = (face_tensor - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "            face_tensor = face_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "            face_tensor = face_tensor.to(device)\n",
    "            \n",
    "            # Make prediction\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(face_tensor)\n",
    "                    emotion_index = torch.argmax(prediction, dim=1).item()\n",
    "                    emotion = emotion_labels[emotion_index]\n",
    "                    \n",
    "                # Draw rectangle around face and display emotion\n",
    "                cv2.rectangle(mirrored_frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                cv2.putText(mirrored_frame, emotion, (x, y - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "            except Exception as e:\n",
    "                print(f'Error during prediction: {e}')\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        frame_count += 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time > 1.0:  # Update FPS every second\n",
    "            fps = frame_count / elapsed_time\n",
    "            frame_count = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # Display FPS on frame\n",
    "        cv2.putText(mirrored_frame, f'FPS: {fps:.2f}', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Emotion Detection (Mirrored)\", mirrored_frame)\n",
    "        \n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('Emotion detection stopped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c1460",
   "metadata": {},
   "source": [
    "## Run the Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the emotion detection\n",
    "detect_emotions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
